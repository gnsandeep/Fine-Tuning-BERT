{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM,Dropout , Input , Bidirectional , concatenate\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "#from tensorflow.keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import SGD\n",
    "import seaborn as sns\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import emoji\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterdata = pd.read_csv(\"dataPandas.csv\")\n",
    "twitterdata.isnull().values.any()\n",
    "twitterdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCounts(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        return len(re.findall(pattern, tweet))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # fit method is used when specific operations need to be done on the train data, but not on the test data\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n",
    "        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n",
    "        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n",
    "        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n",
    "        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?', x))\n",
    "        count_urls = X.apply(lambda x: self.count_regex(r'http.?://[^\\s]+[\\s]?', x))\n",
    "        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n",
    "        # Moreover, it will result in having more words in the tweet\n",
    "        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n",
    "        \n",
    "        df = pd.DataFrame({'count_words': count_words\n",
    "                           , 'count_mentions': count_mentions\n",
    "                           , 'count_hashtags': count_hashtags\n",
    "                           , 'count_capital_words': count_capital_words\n",
    "                           , 'count_excl_quest_marks': count_excl_quest_marks\n",
    "                           , 'count_urls': count_urls\n",
    "                           , 'count_emojis': count_emojis\n",
    "                          })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "tc = TextCounts()\n",
    "twitterdata_eda = tc.fit_transform(twitterdata.Tweets)\n",
    "#twitterdata_eda['Labels'] = twitterdata.Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterdata_meta = twitterdata_eda.to_numpy()\n",
    "twitterdata_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)    \n",
    "   \n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "twitterdata_CT = ct.fit_transform(twitterdata.Tweets)\n",
    "#twitterdata_CT.head()\n",
    "twitterdata['cTweets'] = twitterdata_CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(twitterdata['cTweets'])\n",
    "for sen in sentences:\n",
    "    #X.append(preprocess_text(sen))\n",
    "    X.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = twitterdata['Labels']\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x==\"SARCASM\" else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_train_meta, X_test_meta, y_train_meta, y_test_meta = train_test_split(twitterdata_eda, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "#glove_file = open('./Data/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n",
    "glove_file = open('./Data/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 100, 100)     1968800     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 256)          234496      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            64          input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 264)          0           bidirectional_6[0][0]            \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 264)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            265         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,203,625\n",
      "Trainable params: 234,825\n",
      "Non-trainable params: 1,968,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nlp_input1 = Input(shape=(maxlen,)) \n",
    "meta_input1 = Input(shape=(7,))\n",
    "#emb = Embedding(output_dim=embedding_size, input_dim=100, input_length=seq_length)\n",
    "emb1 = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input1) \n",
    "#lstm = LSTM(300, dropout=0.3, recurrent_dropout=0.3)(embed)\n",
    "#nlp_out2 = Bidirectional(LSTM(128))(meta_input1) \n",
    "x = Dense(8, activation=\"relu\")(meta_input1)\n",
    "\n",
    "\n",
    "nlp_out1 = Bidirectional(LSTM(128))(emb1) \n",
    "concat1 = concatenate([nlp_out1, x]) \n",
    "drop1 = Dropout(0.6)(concat1)\n",
    "#dens = Dense(1)(drop)\n",
    "\n",
    "#classifier = Dense(32, activation='relu')(drop) \n",
    "output1 = Dense(1, activation='sigmoid')(drop1) \n",
    "model_blstm1 = Model(inputs=[nlp_input1 , meta_input1], outputs=[output1])\n",
    "\n",
    "model_blstm1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model_blstm1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 2.5480 - acc: 0.5141 - val_loss: 0.9496 - val_acc: 0.6225\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 6s 252ms/step - loss: 1.8139 - acc: 0.5581 - val_loss: 0.8001 - val_acc: 0.6050\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 7s 261ms/step - loss: 1.2791 - acc: 0.5875 - val_loss: 0.6997 - val_acc: 0.6725\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 7s 260ms/step - loss: 0.9769 - acc: 0.6372 - val_loss: 0.6280 - val_acc: 0.7025\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 6s 257ms/step - loss: 0.7589 - acc: 0.6572 - val_loss: 0.5868 - val_acc: 0.6988\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 6s 251ms/step - loss: 0.6603 - acc: 0.6716 - val_loss: 0.5651 - val_acc: 0.6913\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 6s 252ms/step - loss: 0.5984 - acc: 0.6963 - val_loss: 0.5545 - val_acc: 0.6913\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 7s 262ms/step - loss: 0.5559 - acc: 0.7044 - val_loss: 0.5585 - val_acc: 0.6963\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 6s 256ms/step - loss: 0.5460 - acc: 0.7169 - val_loss: 0.5463 - val_acc: 0.7100\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 6s 260ms/step - loss: 0.5328 - acc: 0.7300 - val_loss: 0.5470 - val_acc: 0.7050\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 6s 254ms/step - loss: 0.5242 - acc: 0.7294 - val_loss: 0.5421 - val_acc: 0.7125\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 6s 253ms/step - loss: 0.5271 - acc: 0.7375 - val_loss: 0.5545 - val_acc: 0.7063\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 6s 253ms/step - loss: 0.5335 - acc: 0.7172 - val_loss: 0.5403 - val_acc: 0.7138\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 7s 262ms/step - loss: 0.5086 - acc: 0.7406 - val_loss: 0.5408 - val_acc: 0.7212\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 6s 257ms/step - loss: 0.4943 - acc: 0.7603 - val_loss: 0.5534 - val_acc: 0.7038\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 6s 252ms/step - loss: 0.4842 - acc: 0.7641 - val_loss: 0.5591 - val_acc: 0.7038\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 7s 261ms/step - loss: 0.4704 - acc: 0.7688 - val_loss: 0.5670 - val_acc: 0.6938\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 7s 261ms/step - loss: 0.4462 - acc: 0.7872 - val_loss: 0.5601 - val_acc: 0.7150\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 7s 261ms/step - loss: 0.4237 - acc: 0.8000 - val_loss: 0.5726 - val_acc: 0.7075\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 7s 264ms/step - loss: 0.4108 - acc: 0.8075 - val_loss: 0.5940 - val_acc: 0.7063\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 7s 264ms/step - loss: 0.3763 - acc: 0.8288 - val_loss: 0.6173 - val_acc: 0.7063\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 6s 254ms/step - loss: 0.3584 - acc: 0.8447 - val_loss: 0.5908 - val_acc: 0.7013\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 6s 255ms/step - loss: 0.3264 - acc: 0.8622 - val_loss: 0.6617 - val_acc: 0.7038\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 6s 251ms/step - loss: 0.2852 - acc: 0.8813 - val_loss: 0.6308 - val_acc: 0.7125\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 6s 251ms/step - loss: 0.2576 - acc: 0.8959 - val_loss: 0.7231 - val_acc: 0.7038\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 7s 262ms/step - loss: 0.2442 - acc: 0.8988 - val_loss: 0.6678 - val_acc: 0.7200\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 6s 251ms/step - loss: 0.2163 - acc: 0.9184 - val_loss: 0.7316 - val_acc: 0.6950\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 6s 253ms/step - loss: 0.2016 - acc: 0.9281 - val_loss: 0.8492 - val_acc: 0.6762\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 6s 257ms/step - loss: 0.1677 - acc: 0.9431 - val_loss: 0.7971 - val_acc: 0.6837\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 6s 260ms/step - loss: 0.1382 - acc: 0.9553 - val_loss: 0.9015 - val_acc: 0.6862\n"
     ]
    }
   ],
   "source": [
    "history = model_blstm1.fit([X_train,X_train_meta ], y_train, batch_size=128, epochs=30, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 100)     1968800     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 256)          234496      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 263)          0           bidirectional[0][0]              \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 263)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            264         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,203,560\n",
      "Trainable params: 234,760\n",
      "Non-trainable params: 1,968,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nlp_input = Input(shape=(maxlen,)) \n",
    "meta_input = Input(shape=(7,))\n",
    "#emb = Embedding(output_dim=embedding_size, input_dim=100, input_length=seq_length)\n",
    "emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input) \n",
    "#lstm = LSTM(300, dropout=0.3, recurrent_dropout=0.3)(embed)\n",
    "\n",
    "nlp_out = Bidirectional(LSTM(128 , dropout=0.3, recurrent_dropout=0.3))(emb) \n",
    "concat = concatenate([nlp_out, meta_input]) \n",
    "drop = Dropout(0.5)(concat)\n",
    "#dens = Dense(1)(drop)\n",
    "\n",
    "#classifier = Dense(32, activation='relu')(drop) \n",
    "output = Dense(1, activation='sigmoid')(drop) \n",
    "model_blstm = Model(inputs=[nlp_input , meta_input], outputs=[output])\n",
    "\n",
    "model_blstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model_blstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - 23s 904ms/step - loss: 1.4659 - acc: 0.4997 - val_loss: 0.7012 - val_acc: 0.5612\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 23s 910ms/step - loss: 0.8647 - acc: 0.5647 - val_loss: 0.6256 - val_acc: 0.6550\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 23s 933ms/step - loss: 0.7346 - acc: 0.6228 - val_loss: 0.5492 - val_acc: 0.7025\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 24s 963ms/step - loss: 0.6589 - acc: 0.6700 - val_loss: 0.5242 - val_acc: 0.7188\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 25s 985ms/step - loss: 0.6121 - acc: 0.6759 - val_loss: 0.5237 - val_acc: 0.7287\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 25s 1s/step - loss: 0.5732 - acc: 0.7081 - val_loss: 0.5200 - val_acc: 0.7225\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 25s 998ms/step - loss: 0.5427 - acc: 0.7109 - val_loss: 0.5230 - val_acc: 0.7237\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.5644 - acc: 0.7066 - val_loss: 0.5311 - val_acc: 0.7300\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 25s 1s/step - loss: 0.5452 - acc: 0.7250 - val_loss: 0.5395 - val_acc: 0.7113\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.5278 - acc: 0.7325 - val_loss: 0.5173 - val_acc: 0.7237\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.5185 - acc: 0.7441 - val_loss: 0.5424 - val_acc: 0.7075\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.5208 - acc: 0.7375 - val_loss: 0.5338 - val_acc: 0.7175\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.5125 - acc: 0.7425 - val_loss: 0.5335 - val_acc: 0.7237\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.5069 - acc: 0.7472 - val_loss: 0.5333 - val_acc: 0.7138\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.4935 - acc: 0.7547 - val_loss: 0.5291 - val_acc: 0.7212\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.4773 - acc: 0.7659 - val_loss: 0.5578 - val_acc: 0.7025\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.4854 - acc: 0.7641 - val_loss: 0.5543 - val_acc: 0.7138\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.4826 - acc: 0.7656 - val_loss: 0.5320 - val_acc: 0.7175\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.4567 - acc: 0.7778 - val_loss: 0.5571 - val_acc: 0.7163\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.4489 - acc: 0.7934 - val_loss: 0.5619 - val_acc: 0.7225\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.4368 - acc: 0.7922 - val_loss: 0.5533 - val_acc: 0.7212\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.4240 - acc: 0.8091 - val_loss: 0.5949 - val_acc: 0.7225\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.4121 - acc: 0.8047 - val_loss: 0.6048 - val_acc: 0.6800\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.4061 - acc: 0.8175 - val_loss: 0.5704 - val_acc: 0.7175\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.3876 - acc: 0.8194 - val_loss: 0.5775 - val_acc: 0.7225\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.3635 - acc: 0.8384 - val_loss: 0.6194 - val_acc: 0.7150\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.3662 - acc: 0.8372 - val_loss: 0.6184 - val_acc: 0.7013\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.3558 - acc: 0.8397 - val_loss: 0.6203 - val_acc: 0.7300\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.3525 - acc: 0.8447 - val_loss: 0.6253 - val_acc: 0.7287\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.3239 - acc: 0.8550 - val_loss: 0.6280 - val_acc: 0.7075\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.2985 - acc: 0.8741 - val_loss: 0.7059 - val_acc: 0.7163\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2956 - acc: 0.8756 - val_loss: 0.6688 - val_acc: 0.7275\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2770 - acc: 0.8878 - val_loss: 0.7625 - val_acc: 0.7212\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2782 - acc: 0.8856 - val_loss: 0.6249 - val_acc: 0.7287\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2508 - acc: 0.8950 - val_loss: 0.7487 - val_acc: 0.6938\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2375 - acc: 0.8997 - val_loss: 0.7747 - val_acc: 0.7075\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2306 - acc: 0.8991 - val_loss: 0.7653 - val_acc: 0.7000\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2214 - acc: 0.9122 - val_loss: 0.7694 - val_acc: 0.7038\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2260 - acc: 0.9094 - val_loss: 0.7216 - val_acc: 0.7113\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2109 - acc: 0.9159 - val_loss: 0.7820 - val_acc: 0.7150\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2261 - acc: 0.9047 - val_loss: 0.6793 - val_acc: 0.7125\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.2495 - acc: 0.8972 - val_loss: 0.7563 - val_acc: 0.6963\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.2028 - acc: 0.9228 - val_loss: 0.8321 - val_acc: 0.6975\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.1825 - acc: 0.9259 - val_loss: 0.8183 - val_acc: 0.7300\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.1754 - acc: 0.9334 - val_loss: 0.8060 - val_acc: 0.7225\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.1614 - acc: 0.9388 - val_loss: 0.8884 - val_acc: 0.6975\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 26s 1s/step - loss: 0.1570 - acc: 0.9378 - val_loss: 0.8509 - val_acc: 0.7063\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.1418 - acc: 0.9488 - val_loss: 0.9383 - val_acc: 0.7050\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.1508 - acc: 0.9397 - val_loss: 0.8728 - val_acc: 0.7113\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 27s 1s/step - loss: 0.1332 - acc: 0.9522 - val_loss: 0.9227 - val_acc: 0.7125\n"
     ]
    }
   ],
   "source": [
    "history = model_blstm.fit([X_train,X_train_meta ], y_train, batch_size=128, epochs=50, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 22ms/step - loss: 0.9499 - acc: 0.6920\n"
     ]
    }
   ],
   "source": [
    "score,accuracy = model_blstm.evaluate([X_test,X_test_meta], y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.9499468803405762\n",
      "Test Accuracy: 0.6919999718666077\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Score:\", score)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata = pd.read_csv(\"dftestdata.csv\")\n",
    "print(twittertestdata.isnull().values.any())\n",
    "twittertestdata_CT = ct.fit_transform(twittertestdata.Tweets)\n",
    "twittertestdata_eda = tc.fit_transform(twittertestdata.Tweets)\n",
    "twittertestdata_meta = twittertestdata_eda.to_numpy()\n",
    "\n",
    "\n",
    "twittertestdata['cTweets'] = twittertestdata_CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = []\n",
    "sentences = list(twittertestdata['cTweets'])\n",
    "for sen in sentences:\n",
    "    X_val.append(sen)\n",
    "    \n",
    "X_validate = tokenizer.texts_to_sequences(X_val)\n",
    "#X_valTokens = tokenizer.texts_to_sequences(X_validate)\n",
    "X_valTokens = pad_sequences(X_validate, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = model_blstm.predict([X_valTokens,twittertestdata_meta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata['Predict'] = validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata['PLabel'] = np.where(twittertestdata['Predict'] > 0.5, \"SARCASM\", \"NOT_SARCASM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertestdata.to_csv('answer_fblstm1.txt', columns = [\"ID\" , \"PLabel\"] , header = False , index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
