{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Keras_FunctionalBLSTM.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DU5x6Pfv-nx",
        "outputId": "ee336ff5-6690-4096-98f5-1e77c9fdafa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install emoji\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "\n",
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential \n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM,Dropout , Input , Bidirectional , concatenate\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "#from tensorflow.keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.optimizers import SGD\n",
        "import seaborn as sns\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import emoji\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import json\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiMkZKRJwAap"
      },
      "source": [
        "def load_training_data_to_pandas(filename = 'train.jsonl'):\r\n",
        "    X = []\r\n",
        "    Y = []\r\n",
        "    fhand = open(filename,encoding='utf8')\r\n",
        "    for line in fhand:\r\n",
        "        data = json.loads(line)\r\n",
        "\r\n",
        "        lt = data['context']\r\n",
        "        lt.reverse()\r\n",
        "        fullTweet =   data['response'] + \" \" + ''.join(lt)\r\n",
        "\r\n",
        "        X.append(fullTweet)\r\n",
        "        Y.append(data['label'])\r\n",
        " \r\n",
        "    \r\n",
        "    return pd.DataFrame({'Tweets': X,'Labels': Y})\r\n",
        "\r\n",
        "def load_test_data_to_pandas(filename = 'test.jsonl'):\r\n",
        "    tid = []\r\n",
        "    X = []\r\n",
        "    Y = []\r\n",
        "    fhand = open(filename,encoding='utf8')\r\n",
        "    for line in fhand:\r\n",
        "        data = json.loads(line)\r\n",
        "        tid.append(data['id'])\r\n",
        "        lt = data['context']\r\n",
        "        lt.reverse()\r\n",
        "        fullTweet =   data['response'] + \" \" + ''.join(lt)\r\n",
        "\r\n",
        "        X.append(fullTweet)\r\n",
        "        \r\n",
        "    \r\n",
        "    return  pd.DataFrame({'ID': tid,\r\n",
        "                   'Tweets': X})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByAVu1WnwFFE"
      },
      "source": [
        "twitterdata = load_training_data_to_pandas()\r\n",
        "twittertestdata = load_test_data_to_pandas()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxGpmXwZv-nz",
        "outputId": "cf1efacc-0280-472a-9e32-27286c66913c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#twitterdata = pd.read_csv(\"dataPandas.csv\")\n",
        "twitterdata.isnull().values.any()\n",
        "twitterdata.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JvkpAriv-n0"
      },
      "source": [
        "class TextCounts(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def count_regex(self, pattern, tweet):\n",
        "        return len(re.findall(pattern, tweet))\n",
        "    \n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        # fit method is used when specific operations need to be done on the train data, but not on the test data\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, **transform_params):\n",
        "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n",
        "        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n",
        "        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n",
        "        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n",
        "        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?', x))\n",
        "        count_urls = X.apply(lambda x: self.count_regex(r'http.?://[^\\s]+[\\s]?', x))\n",
        "        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n",
        "        # Moreover, it will result in having more words in the tweet\n",
        "        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n",
        "        \n",
        "        df = pd.DataFrame({'count_words': count_words\n",
        "                           , 'count_mentions': count_mentions\n",
        "                           , 'count_hashtags': count_hashtags\n",
        "                           , 'count_capital_words': count_capital_words\n",
        "                           , 'count_excl_quest_marks': count_excl_quest_marks\n",
        "                           , 'count_urls': count_urls\n",
        "                           , 'count_emojis': count_emojis\n",
        "                          })\n",
        "        \n",
        "        return df\n",
        "    \n",
        "tc = TextCounts()\n",
        "twitterdata_eda = tc.fit_transform(twitterdata.Tweets)\n",
        "#twitterdata_eda['Labels'] = twitterdata.Labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvEnt3ZGv-n0",
        "outputId": "4e7a6a49-7d7e-47a7-f6ca-ca6da877e906",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "twitterdata_meta = twitterdata_eda.to_numpy()\n",
        "twitterdata_meta.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqu8_3Eav-n1"
      },
      "source": [
        "class CleanText(BaseEstimator, TransformerMixin):\n",
        "    def remove_mentions(self, input_text):\n",
        "        return re.sub(r'@\\w+', '', input_text)\n",
        "    \n",
        "    def remove_urls(self, input_text):\n",
        "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
        "    \n",
        "    def emoji_oneword(self, input_text):\n",
        "        # By compressing the underscore, the emoji is kept as one word\n",
        "        return input_text.replace('_','')\n",
        "    \n",
        "    def remove_punctuation(self, input_text):\n",
        "        # Make translation table\n",
        "        punct = string.punctuation\n",
        "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
        "        return input_text.translate(trantab)    \n",
        "   \n",
        "    def remove_digits(self, input_text):\n",
        "        return re.sub('\\d+', '', input_text)\n",
        "    \n",
        "    def to_lower(self, input_text):\n",
        "        return input_text.lower()\n",
        "    \n",
        "    def remove_stopwords(self, input_text):\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "        whitelist = [\"n't\", \"not\", \"no\"]\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def stemming(self, input_text):\n",
        "        porter = PorterStemmer()\n",
        "        words = input_text.split() \n",
        "        stemmed_words = [porter.stem(word) for word in words]\n",
        "        return \" \".join(stemmed_words)\n",
        "    \n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, **transform_params):\n",
        "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n",
        "        return clean_X"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efq9eEehv-n2"
      },
      "source": [
        "ct = CleanText()\n",
        "twitterdata_CT = ct.fit_transform(twitterdata.Tweets)\n",
        "#twitterdata_CT.head()\n",
        "twitterdata['cTweets'] = twitterdata_CT"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU-V1jX6v-n2"
      },
      "source": [
        "X = []\n",
        "sentences = list(twitterdata['cTweets'])\n",
        "for sen in sentences:\n",
        "    #X.append(preprocess_text(sen))\n",
        "    X.append(sen)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiE0v-74v-n2"
      },
      "source": [
        "y = twitterdata['Labels']\n",
        "y = np.array(list(map(lambda x: 1 if x==\"SARCASM\" else 0, y)))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjLR3y-xv-n2"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "X_train_meta, X_test_meta, y_train_meta, y_test_meta = train_test_split(twitterdata_eda, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCwjlgvUv-n3"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39DvjUaK1Mes",
        "outputId": "4ae4b21b-8c1f-47da-a5b5-b2c868f20ed0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-09 23:27:32--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-12-09 23:27:32--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-12-09 23:27:33--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.01MB/s    in 6m 59s  \n",
            "\n",
            "2020-12-09 23:34:32 (1.96 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wseuBT081Tdt"
      },
      "source": [
        "import zipfile\r\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\r\n",
        "zip_ref.extractall('.')\r\n",
        "zip_ref.close()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn7_Bgrev-n3"
      },
      "source": [
        "embeddings_dictionary = dict()\n",
        "#glove_file = open('./Data/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n",
        "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc0-RWKFv-n4"
      },
      "source": [
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-oMEEB7v-n5",
        "outputId": "3a4318ac-1dca-431e-d062-7d13f44cddc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp_input = Input(shape=(maxlen,)) \n",
        "meta_input = Input(shape=(7,))\n",
        "#emb = Embedding(output_dim=embedding_size, input_dim=100, input_length=seq_length)\n",
        "emb = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)(nlp_input) \n",
        "#lstm = LSTM(300, dropout=0.3, recurrent_dropout=0.3)(embed)\n",
        "\n",
        "nlp_out = Bidirectional(LSTM(128 , dropout=0.3, recurrent_dropout=0.3))(emb) \n",
        "concat = concatenate([nlp_out, meta_input]) \n",
        "drop = Dropout(0.5)(concat)\n",
        "#dens = Dense(1)(drop)\n",
        "\n",
        "#classifier = Dense(32, activation='relu')(drop) \n",
        "output = Dense(1, activation='sigmoid')(drop) \n",
        "model_blstm = Model(inputs=[nlp_input , meta_input], outputs=[output])\n",
        "\n",
        "model_blstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(model_blstm.summary())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 100, 100)     1968800     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 256)          234496      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 263)          0           bidirectional[0][0]              \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 263)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            264         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,203,560\n",
            "Trainable params: 234,760\n",
            "Non-trainable params: 1,968,800\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpbxpT1fv-n6",
        "outputId": "4dfea0c2-a32c-4f9b-dc38-9f5c1ed41afe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model_blstm.fit([X_train,X_train_meta ], y_train, batch_size=128, epochs=50, verbose=1, validation_split=0.2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "25/25 [==============================] - 24s 942ms/step - loss: 6.1495 - acc: 0.4959 - val_loss: 3.0134 - val_acc: 0.4600\n",
            "Epoch 2/50\n",
            "25/25 [==============================] - 23s 923ms/step - loss: 4.6903 - acc: 0.4875 - val_loss: 2.1598 - val_acc: 0.4675\n",
            "Epoch 3/50\n",
            "25/25 [==============================] - 23s 935ms/step - loss: 2.7009 - acc: 0.5238 - val_loss: 1.0577 - val_acc: 0.5750\n",
            "Epoch 4/50\n",
            "25/25 [==============================] - 23s 933ms/step - loss: 1.3950 - acc: 0.5456 - val_loss: 0.6737 - val_acc: 0.6500\n",
            "Epoch 5/50\n",
            "25/25 [==============================] - 23s 918ms/step - loss: 0.7425 - acc: 0.6309 - val_loss: 0.5902 - val_acc: 0.6725\n",
            "Epoch 6/50\n",
            "25/25 [==============================] - 23s 924ms/step - loss: 0.6396 - acc: 0.6581 - val_loss: 0.6331 - val_acc: 0.6438\n",
            "Epoch 7/50\n",
            "25/25 [==============================] - 23s 917ms/step - loss: 0.5839 - acc: 0.6878 - val_loss: 0.5390 - val_acc: 0.7050\n",
            "Epoch 8/50\n",
            "25/25 [==============================] - 23s 932ms/step - loss: 0.5793 - acc: 0.6897 - val_loss: 0.5418 - val_acc: 0.7100\n",
            "Epoch 9/50\n",
            "25/25 [==============================] - 23s 920ms/step - loss: 0.5540 - acc: 0.7066 - val_loss: 0.5515 - val_acc: 0.6988\n",
            "Epoch 10/50\n",
            "25/25 [==============================] - 23s 927ms/step - loss: 0.5479 - acc: 0.7144 - val_loss: 0.5433 - val_acc: 0.7050\n",
            "Epoch 11/50\n",
            "25/25 [==============================] - 23s 917ms/step - loss: 0.5384 - acc: 0.7203 - val_loss: 0.5276 - val_acc: 0.7200\n",
            "Epoch 12/50\n",
            "25/25 [==============================] - 25s 997ms/step - loss: 0.5386 - acc: 0.7234 - val_loss: 0.5340 - val_acc: 0.7212\n",
            "Epoch 13/50\n",
            "25/25 [==============================] - 23s 938ms/step - loss: 0.5391 - acc: 0.7184 - val_loss: 0.5428 - val_acc: 0.7050\n",
            "Epoch 14/50\n",
            "25/25 [==============================] - 24s 960ms/step - loss: 0.5334 - acc: 0.7272 - val_loss: 0.5346 - val_acc: 0.7113\n",
            "Epoch 15/50\n",
            "25/25 [==============================] - 24s 942ms/step - loss: 0.5243 - acc: 0.7375 - val_loss: 0.5341 - val_acc: 0.7063\n",
            "Epoch 16/50\n",
            "25/25 [==============================] - 24s 948ms/step - loss: 0.5240 - acc: 0.7312 - val_loss: 0.5313 - val_acc: 0.7188\n",
            "Epoch 17/50\n",
            "25/25 [==============================] - 23s 938ms/step - loss: 0.5164 - acc: 0.7325 - val_loss: 0.5358 - val_acc: 0.7150\n",
            "Epoch 18/50\n",
            "25/25 [==============================] - 23s 936ms/step - loss: 0.5206 - acc: 0.7366 - val_loss: 0.5367 - val_acc: 0.7150\n",
            "Epoch 19/50\n",
            "25/25 [==============================] - 23s 922ms/step - loss: 0.4992 - acc: 0.7425 - val_loss: 0.5347 - val_acc: 0.7063\n",
            "Epoch 20/50\n",
            "25/25 [==============================] - 24s 950ms/step - loss: 0.4984 - acc: 0.7550 - val_loss: 0.5155 - val_acc: 0.7337\n",
            "Epoch 21/50\n",
            "25/25 [==============================] - 23s 927ms/step - loss: 0.5000 - acc: 0.7503 - val_loss: 0.5173 - val_acc: 0.7412\n",
            "Epoch 22/50\n",
            "25/25 [==============================] - 23s 926ms/step - loss: 0.4887 - acc: 0.7519 - val_loss: 0.5275 - val_acc: 0.7300\n",
            "Epoch 23/50\n",
            "25/25 [==============================] - 23s 917ms/step - loss: 0.4872 - acc: 0.7566 - val_loss: 0.5588 - val_acc: 0.7212\n",
            "Epoch 24/50\n",
            "25/25 [==============================] - 23s 916ms/step - loss: 0.4848 - acc: 0.7616 - val_loss: 0.5162 - val_acc: 0.7437\n",
            "Epoch 25/50\n",
            "25/25 [==============================] - 23s 928ms/step - loss: 0.4682 - acc: 0.7750 - val_loss: 0.5269 - val_acc: 0.7287\n",
            "Epoch 26/50\n",
            "25/25 [==============================] - 23s 920ms/step - loss: 0.4696 - acc: 0.7750 - val_loss: 0.5318 - val_acc: 0.7350\n",
            "Epoch 27/50\n",
            "25/25 [==============================] - 23s 931ms/step - loss: 0.4487 - acc: 0.7812 - val_loss: 0.5279 - val_acc: 0.7350\n",
            "Epoch 28/50\n",
            "25/25 [==============================] - 23s 935ms/step - loss: 0.4480 - acc: 0.7847 - val_loss: 0.5446 - val_acc: 0.7262\n",
            "Epoch 29/50\n",
            "25/25 [==============================] - 24s 950ms/step - loss: 0.4329 - acc: 0.7991 - val_loss: 0.5234 - val_acc: 0.7450\n",
            "Epoch 30/50\n",
            "25/25 [==============================] - 23s 929ms/step - loss: 0.4205 - acc: 0.8094 - val_loss: 0.5760 - val_acc: 0.7275\n",
            "Epoch 31/50\n",
            "25/25 [==============================] - 23s 932ms/step - loss: 0.4147 - acc: 0.8100 - val_loss: 0.5213 - val_acc: 0.7287\n",
            "Epoch 32/50\n",
            "25/25 [==============================] - 23s 924ms/step - loss: 0.4129 - acc: 0.8128 - val_loss: 0.5194 - val_acc: 0.7362\n",
            "Epoch 33/50\n",
            "25/25 [==============================] - 23s 920ms/step - loss: 0.3889 - acc: 0.8163 - val_loss: 0.5485 - val_acc: 0.7300\n",
            "Epoch 34/50\n",
            "25/25 [==============================] - 23s 921ms/step - loss: 0.3642 - acc: 0.8400 - val_loss: 0.5633 - val_acc: 0.7337\n",
            "Epoch 35/50\n",
            "25/25 [==============================] - 23s 924ms/step - loss: 0.3759 - acc: 0.8244 - val_loss: 0.5658 - val_acc: 0.7262\n",
            "Epoch 36/50\n",
            "25/25 [==============================] - 23s 921ms/step - loss: 0.3588 - acc: 0.8359 - val_loss: 0.5746 - val_acc: 0.7362\n",
            "Epoch 37/50\n",
            "25/25 [==============================] - 25s 981ms/step - loss: 0.3280 - acc: 0.8619 - val_loss: 0.6125 - val_acc: 0.7412\n",
            "Epoch 38/50\n",
            "25/25 [==============================] - 23s 920ms/step - loss: 0.3120 - acc: 0.8669 - val_loss: 0.6187 - val_acc: 0.7250\n",
            "Epoch 39/50\n",
            "25/25 [==============================] - 23s 918ms/step - loss: 0.3187 - acc: 0.8684 - val_loss: 0.6178 - val_acc: 0.7262\n",
            "Epoch 40/50\n",
            "25/25 [==============================] - 23s 921ms/step - loss: 0.3010 - acc: 0.8737 - val_loss: 0.6539 - val_acc: 0.7350\n",
            "Epoch 41/50\n",
            "25/25 [==============================] - 23s 917ms/step - loss: 0.2937 - acc: 0.8803 - val_loss: 0.6478 - val_acc: 0.7275\n",
            "Epoch 42/50\n",
            "25/25 [==============================] - 23s 934ms/step - loss: 0.2841 - acc: 0.8850 - val_loss: 0.6252 - val_acc: 0.7362\n",
            "Epoch 43/50\n",
            "25/25 [==============================] - 23s 920ms/step - loss: 0.2642 - acc: 0.8934 - val_loss: 0.6444 - val_acc: 0.7262\n",
            "Epoch 44/50\n",
            "25/25 [==============================] - 23s 923ms/step - loss: 0.2580 - acc: 0.8909 - val_loss: 0.6880 - val_acc: 0.7375\n",
            "Epoch 45/50\n",
            "25/25 [==============================] - 23s 931ms/step - loss: 0.2491 - acc: 0.9019 - val_loss: 0.6477 - val_acc: 0.7487\n",
            "Epoch 46/50\n",
            "25/25 [==============================] - 23s 937ms/step - loss: 0.2376 - acc: 0.9013 - val_loss: 0.6438 - val_acc: 0.7362\n",
            "Epoch 47/50\n",
            "25/25 [==============================] - 23s 927ms/step - loss: 0.2330 - acc: 0.9087 - val_loss: 0.7225 - val_acc: 0.7362\n",
            "Epoch 48/50\n",
            "25/25 [==============================] - 23s 920ms/step - loss: 0.2342 - acc: 0.9059 - val_loss: 0.7395 - val_acc: 0.7200\n",
            "Epoch 49/50\n",
            "25/25 [==============================] - 23s 921ms/step - loss: 0.2105 - acc: 0.9144 - val_loss: 0.7748 - val_acc: 0.7150\n",
            "Epoch 50/50\n",
            "25/25 [==============================] - 23s 929ms/step - loss: 0.2080 - acc: 0.9219 - val_loss: 0.8230 - val_acc: 0.7237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekqMXDzov-n6",
        "outputId": "b9d83f4f-ae87-4aac-958a-529f8de0bbd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "score,accuracy = model_blstm.evaluate([X_test,X_test_meta], y_test, verbose=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 2s 62ms/step - loss: 0.8410 - acc: 0.7060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_O7LwfVv-n7",
        "outputId": "47402597-2478-4e26-cc05-e43e96949d42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Test Score:\", score)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score: 0.8410437703132629\n",
            "Test Accuracy: 0.7059999704360962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8i73Smjv-n7",
        "outputId": "7a2fe92a-35ab-4426-e651-0d7fcb94911a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#twittertestdata = pd.read_csv(\"dftestdata.csv\")\n",
        "print(twittertestdata.isnull().values.any())\n",
        "twittertestdata_CT = ct.fit_transform(twittertestdata.Tweets)\n",
        "twittertestdata_eda = tc.fit_transform(twittertestdata.Tweets)\n",
        "twittertestdata_meta = twittertestdata_eda.to_numpy()\n",
        "\n",
        "\n",
        "twittertestdata['cTweets'] = twittertestdata_CT"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwHq_MHvv-n7"
      },
      "source": [
        "X_val = []\n",
        "sentences = list(twittertestdata['cTweets'])\n",
        "for sen in sentences:\n",
        "    X_val.append(sen)\n",
        "    \n",
        "X_validate = tokenizer.texts_to_sequences(X_val)\n",
        "#X_valTokens = tokenizer.texts_to_sequences(X_validate)\n",
        "X_valTokens = pad_sequences(X_validate, padding='post', maxlen=maxlen)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8Cn9j9Kv-n8"
      },
      "source": [
        "validation = model_blstm.predict([X_valTokens,twittertestdata_meta])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bohYuirAv-n8"
      },
      "source": [
        "twittertestdata['Predict'] = validation"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWSvfqcsv-n8"
      },
      "source": [
        "twittertestdata['PLabel'] = np.where(twittertestdata['Predict'] > 0.5, \"SARCASM\", \"NOT_SARCASM\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQQprPRhv-n8",
        "outputId": "fc96546f-7f69-4bda-ca54-d3dc4dc4ca8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "twittertestdata.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Tweets</th>\n",
              "      <th>cTweets</th>\n",
              "      <th>Predict</th>\n",
              "      <th>PLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>twitter_1</td>\n",
              "      <td>@USER @USER @USER My 3 year old , that just fi...</td>\n",
              "      <td>year old finish read nietzsch ask ayo papa peo...</td>\n",
              "      <td>0.002249</td>\n",
              "      <td>NOT_SARCASM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>twitter_2</td>\n",
              "      <td>@USER @USER How many verifiable lies has he to...</td>\n",
              "      <td>mani verifi lie told document truth teller sur...</td>\n",
              "      <td>0.993121</td>\n",
              "      <td>SARCASM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>twitter_3</td>\n",
              "      <td>@USER @USER @USER Maybe Docs just a scrub of a...</td>\n",
              "      <td>mayb doc scrub coach mean get hammer gold stan...</td>\n",
              "      <td>0.688716</td>\n",
              "      <td>SARCASM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>twitter_4</td>\n",
              "      <td>@USER @USER is just a cover up for the real ha...</td>\n",
              "      <td>cover real hate insid left nutshel url hate pl...</td>\n",
              "      <td>0.548973</td>\n",
              "      <td>SARCASM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>twitter_5</td>\n",
              "      <td>@USER @USER @USER The irony being that he even...</td>\n",
              "      <td>ironi even ask quit articul consid comment fin...</td>\n",
              "      <td>0.987163</td>\n",
              "      <td>SARCASM</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  ...       PLabel\n",
              "0  twitter_1  ...  NOT_SARCASM\n",
              "1  twitter_2  ...      SARCASM\n",
              "2  twitter_3  ...      SARCASM\n",
              "3  twitter_4  ...      SARCASM\n",
              "4  twitter_5  ...      SARCASM\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE7d9hDFv-n8"
      },
      "source": [
        "twittertestdata.to_csv('answer_fblstm1.txt', columns = [\"ID\" , \"PLabel\"] , header = False , index = False)"
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}